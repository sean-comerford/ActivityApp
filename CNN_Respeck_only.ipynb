{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzd9Z9OZxNvd"
   },
   "source": [
    "# **Machine Learning Model**\n",
    "\n",
    "This notebook implements a convolutional neural network to recognise different physical activities from Respeck sensor data. The dataset includes multiple 30-second recordings of various physical activities (e.g., ascending stairs, shuffle walking, sitting-standing) stored in separate CSV files for each activity.\n",
    "\n",
    "This model will be deployed inside the Android app for live classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXyHZD1A0X7J"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "s2B8Hymdj1Sg"
   },
   "outputs": [],
   "source": [
    "# Importing libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icbrBf1Kl6vp"
   },
   "source": [
    "# Reading Files\n",
    "Reading files from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pTsJd33Kl44J"
   },
   "outputs": [],
   "source": [
    "# Path to Respeck data\n",
    "your_dataset_path = \"./PDIoT2324/Respeck/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOTe3o9Il4ST"
   },
   "source": [
    "This line uses the glob module to find all file paths that match a specified pattern. The 'glob.glob()' function returns a list of file paths that match the given pattern. `your_dataset_path` should be the directory where your dataset files are located.\n",
    "\n",
    "The `*` is a wildcard character that matches any string of characters,  so this pattern retrieves all folders in the 'your_dataset_path' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4izGxKkllz6",
    "outputId": "49f7031e-36ae-454a-a8dc-4a7e92243315"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./PDIoT2324/Respeck\\\\s100_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s100_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s101_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s102_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s10_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s11_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s12_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s13_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s14_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s15_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s16_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s17_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s18_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s1_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s20_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s21_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s22_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s23_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s24_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s26_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s27_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s28_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_sitting_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s29_respeck_standing_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_ascending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_descending_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingBack_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingLeft_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingRight_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_hyperventilating.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_laughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_singing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_lyingStomach_talking.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_miscMovement_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_normalWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_running_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_shuffleWalking_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_sitting_breathingNormal.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_sitting_coughing.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_sitting_eating.csv',\n",
       " './PDIoT2324/Respeck\\\\s30_respeck_sitting_hyperventilating.csv',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(your_dataset_path + \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities and Social Signals Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activities and social signals with corresponding labels\n",
    "# Each key is the name of the physical activity, and the corresponding value is the numeric label\n",
    "# These labels will be used as the target variable for classification\n",
    "activities_dict = {\n",
    "    'ascending': 0,\n",
    "    'shuffleWalking': 1,\n",
    "    'sittingStanding': 2,\n",
    "    'miscMovement': 3,\n",
    "    'normalWalking': 4,\n",
    "    'lyingBack': 5,\n",
    "    'lyingLeft': 6,\n",
    "    'lyingRight': 7,\n",
    "    'lyingStomach': 8,\n",
    "    'descending': 9,\n",
    "    'running': 10\n",
    "}\n",
    "\n",
    "social_signals_dict = {\n",
    "    'breathingNormal': 0,\n",
    "    'coughing': 1,\n",
    "    'hyperventilating': 2,\n",
    "    'other': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7eNuiHKmBuT"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zdg12YooOJF"
   },
   "source": [
    "## Load list of files in an activity folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "b_ZtuAb64ZsD"
   },
   "outputs": [],
   "source": [
    "def load_files_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all CSV files from a folder, extract activity and social signal information,\n",
    "    and return a list of file paths along with combined labels and file information.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - list: A list of file paths for all CSV files in the folder.\n",
    "        - list: A list of combined labels (activity and social signal) for each file.\n",
    "        - dict: A dictionary containing file information with activity and social signal labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise an empty list to store the full file paths of the CSV files\n",
    "    file_paths = []\n",
    "\n",
    "    # Initialise an empty dictionary to store the filenames and the activity and social signal labels\n",
    "    file_info = {}\n",
    "\n",
    "    # Initialise an empty list to store the combined labels of the activity and social signal to be used in stratified split of data\n",
    "    # Stratified split of data ensures that each activity-social signal combination is proportionally represented in the training and testing data\n",
    "    combined_labels = []\n",
    "\n",
    "    # Loop through all the files in the given folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        \n",
    "        # Check if the file has a .csv extension (ignores other files)\n",
    "        if file_name.endswith('.csv'):\n",
    "\n",
    "            # Inialise an empty dictionary for each file to store the activity and social signal labels\n",
    "            file_info[file_name] = {}\n",
    "\n",
    "            # Split the file name by underscores to extract activity and social signal information\n",
    "            parts = file_name.split(\"_\")\n",
    "            \n",
    "            # Extract the activity from the file name\n",
    "            activity = parts[2]\n",
    "\n",
    "            # Add activity label to the file_info dictionary\n",
    "            if activity == \"sitting\" or activity == \"standing\":\n",
    "                file_info[file_name]['activity_label'] = activities_dict[\"sittingStanding\"]\n",
    "            else:\n",
    "                file_info[file_name]['activity_label'] = activities_dict[activity]\n",
    "            \n",
    "            # Extract the social signal from the file name, without the .csv extension\n",
    "            social_signal = parts[3].split(\".\")[0]\n",
    "\n",
    "            # Add social signal label to the file_info dictionary\n",
    "            if social_signal == \"laughing\" or social_signal == \"eating\" or social_signal == \"talking\" or social_signal == \"singing\":\n",
    "                file_info[file_name]['social_signal_label'] = social_signals_dict[\"other\"]\n",
    "            else:\n",
    "                file_info[file_name]['social_signal_label'] = social_signals_dict[social_signal]\n",
    "            \n",
    "            # Combine the activity and social signal to create a unique label\n",
    "            combined_label = activity + \"_\" + social_signal\n",
    "\n",
    "            # Append the combined label to the combined_labels list\n",
    "            combined_labels.append(combined_label)\n",
    "\n",
    "            # Construct the full file path by joining the folder path and the file name'\n",
    "            full_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Append the full file path to the file_paths list\n",
    "            file_paths.append(full_file_path)\n",
    "\n",
    "    # Return the complete list of CSV file paths\n",
    "    return file_paths, combined_labels, file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUGBeBBn_L8"
   },
   "source": [
    "## Train and test set split from list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2SzHoQz2NH3v"
   },
   "outputs": [],
   "source": [
    "def split_files(file_list, combined_labels, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the list of file paths into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list): A list of file paths for all CSV files.\n",
    "    combined_labels (list): A list of combined labels (activity and social signal) for each file.\n",
    "    test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - list: Training file paths.\n",
    "        - list: Testing file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the file list into training and test sets using train_test_split from scikit-learn\n",
    "    # test_size defines the proportion of the data to use as the test set (default is 20%)\n",
    "    # shuffle=True ensures that the files are shuffled randomly before splitting\n",
    "    # stratify is used to ensure that the proportion of each class is the same in both the training and testing sets\n",
    "    train_files, test_files = train_test_split(file_list, test_size=test_size, stratify = combined_labels, shuffle=True, random_state=42)\n",
    "\n",
    "    # Return the train and test file lists\n",
    "    return train_files, test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J7-zQgZzP19"
   },
   "source": [
    "## Sliding Window\n",
    "\n",
    "In time series Activity Recognition, a sliding window is a commonly used technique to segment continuous sensor data (such as accelerometer readings) into smaller, fixed-length overlapping or non-overlapping time intervals, or windows. Each window contains a sequence of sensor measurements that represent a short period of time, and this segmented data is used to extract features or make predictions about the activity happening within that window.\n",
    "\n",
    "### Key Concepts of a Sliding Window\n",
    "1.   **Window Size:** This refers to the length of each segment or window, typically defined in terms of the number of time steps or the duration (e.g., 2 seconds). The window size should be chosen carefully to capture enough information about the activity without making the window too large.\n",
    "2.   **Step Size:** The step size determines how far the window moves forward after each step. If the step size is smaller than the window size, the windows will overlap. For example, if the window size is 5 seconds and the step size is 2 seconds, there will be a 3-second overlap between consecutive windows. Overlapping windows provide more data for analysis and can help smooth out predictions by capturing transitional activities.\n",
    "3.   **Non-Overlapping Windows:** If the step size is equal to the window size, the windows do not overlap. This method provides distinct segments of data but may miss transitional phases between activities.\n",
    "\n",
    "### Why Sliding Windows for Activity Recognition?\n",
    "\n",
    "* Segmentation of Continuous Data: Activity recognition systems work with continuous streams of sensor data, and the sliding window helps segment these into manageable pieces to classify activities within specific intervals.\n",
    "\n",
    "* Context Capturing: Human activities are often complex and spread across time. By using a sliding window, you can capture context across a short duration, which may include transitions or small fluctuations in the activity (e.g., a person moving from sitting to standing).\n",
    "\n",
    "* Feature Extraction: Within each window, features such as mean, variance, frequency domain features, etc., can be extracted to help classify the activity.\n",
    "\n",
    "* Real-Time Recognition: In real-time systems, the sliding window allows for continuous monitoring and updating of predictions as new data arrives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "u3SuHww6MpEx"
   },
   "outputs": [],
   "source": [
    "def load_and_apply_sliding_windows(file_path, window_size, step_size, file_info):\n",
    "    \"\"\"\n",
    "    Load the data from each file, apply sliding windows, and return the windows and labels.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list): List of file paths to CSV files. Each file contains sensor data.\n",
    "    window_size (int): The size of each sliding window (number of time steps).\n",
    "    step_size (int): The step size (stride) between consecutive windows.\n",
    "    file_info (dict): Dictionary containing file information with activity and social signal labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - windows (numpy.ndarray): A 3D array of sliding windows, where each window has the shape\n",
    "                                   (num_windows, window_size, num_features).\n",
    "        - activity_labels (numpy.ndarray): A 1D array of activity labels, where each label corresponds to a sliding window.\n",
    "        - social_signal_labels (numpy.ndarray): A 1D array of social signal labels, where each label corresponds to a sliding window.\n",
    "    \"\"\"\n",
    "    # Initialise lists to store sliding windows and their corresponding labels\n",
    "    windows = []\n",
    "    activity_labels = []\n",
    "    social_signal_labels = []\n",
    "    file_number = 0\n",
    "\n",
    "    \n",
    "    # Loop through each file in the provided file path\n",
    "    for file in file_path:\n",
    "\n",
    "        # Extract the activity and social signal labels from the file_info dictionary\n",
    "        activity_label = file_info[os.path.basename(file)]['activity_label']\n",
    "        social_signal_label = file_info[os.path.basename(file)]['social_signal_label']\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        data = pd.read_csv(file)   \n",
    "\n",
    "\n",
    "        # Select the columns containing the necessary sensor data (acceleration readings)\n",
    "        # These columns might vary depending on your dataset's structure\n",
    "        data = data[['accel_x', 'accel_y', 'accel_z']]\n",
    "        \n",
    "        # Convert the DataFrame into a numpy array for faster processing in the sliding window operation\n",
    "        data = data.to_numpy()\n",
    "\n",
    "        \n",
    "        # Get the number of samples (rows) and features (columns) in the data\n",
    "        num_samples, num_features = data.shape\n",
    "        \n",
    "        # Apply sliding windows to the data\n",
    "        # The range function defines the start of each window, moving step_size increments at a time\n",
    "        for i in range(0, num_samples - window_size + 1, step_size):\n",
    "            # Extract a window of size 'window_size' from the current position 'i'\n",
    "            window = data[i:i + window_size, :]\n",
    "\n",
    "            # Append the window to the windows list\n",
    "            windows.append(window)\n",
    "\n",
    "            # Assign the activity label to the window and append it to the activity labels list\n",
    "            activity_labels.append(activity_label)\n",
    "\n",
    "            # Assign the social signal label to the window and append it to the social signal labels list\n",
    "            social_signal_labels.append(social_signal_label)\n",
    "\n",
    "    # Convert the lists of windows and labels into numpy arrays for efficient numerical operations\n",
    "    return np.array(windows), np.array(activity_labels), np.array(social_signal_labels) \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Ku5P4Lm8QA"
   },
   "source": [
    "## Load and Split Train Test for Each Activity Folder\n",
    "\n",
    "This function processes the sensor data for a specific activity, such as 'walking' or 'running', stored in its respective folder. It splits the data into training and testing sets, applies sliding windows, and labels the windows with the corresponding activity. This function can be used repeatedly for each activity to process and prepare data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zBVvTBi7N_fh"
   },
   "outputs": [],
   "source": [
    "def process_activity(dataset_path, window_size=50, step_size=50, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Processes an activity folder by loading the file list, splitting them into\n",
    "    train and test sets, and applying sliding windows to the files.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Base path where the activity folders are located.\n",
    "        window_size (int): Size of the sliding window, i.e., the number of time steps included in each window.\n",
    "                           Default is 50.\n",
    "        step_size (int): Step size for the sliding window, i.e., how far the window moves along the data.\n",
    "                         Default is 50 (no overlap between windows).\n",
    "        test_size (float): Proportion of files to use for testing. Default is 0.2, meaning 20% of files will\n",
    "                           be allocated to the test set.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - train_windows (numpy.ndarray): Sliding windows from the training files.\n",
    "            - train_activity_labels (numpy.ndarray): Corresponding activity labels for the training windows.\n",
    "            - train_social_signal_labels (numpy.ndarray): Corresponding social signal labels for the training windows.\n",
    "            - test_windows (numpy.ndarray): Sliding windows from the test files.\n",
    "            - test_activity_labels (numpy.ndarray): Corresponding activity labels for the test windows.\n",
    "            - test_social_signal_labels (numpy.ndarray): Corresponding social signal labels for the test windows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load all CSV file paths for the given activity from the folder\n",
    "    file_paths, combined_labels, file_info = load_files_from_folder(dataset_path)\n",
    "\n",
    "    # Split the file list into training and testing sets\n",
    "    # train_files: files used for training\n",
    "    # test_files: files used for testing\n",
    "    train_files, test_files = split_files(file_paths, combined_labels, test_size=test_size)\n",
    "\n",
    "    # Apply sliding windows to the training files\n",
    "    # The function 'load_and_apply_sliding_windows' returns the sliding windows (segments) and their corresponding activity and social signal labels\n",
    "    train_windows, train_activity_labels, train_social_signal_labels = load_and_apply_sliding_windows(train_files, window_size, step_size, file_info)\n",
    "\n",
    "    # Apply sliding windows to the testing files\n",
    "    test_windows, test_activity_labels, test_social_signal_labels = load_and_apply_sliding_windows(test_files, window_size, step_size, file_info)\n",
    "\n",
    "    # Return the sliding windows and their labels for both training and testing sets\n",
    "    return train_windows, train_activity_labels, train_social_signal_labels, test_windows, test_activity_labels, test_social_signal_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wv1PuOLgUV8"
   },
   "source": [
    "## 1D CNN Model\n",
    "\n",
    "This function, `build_1d_cnn_model`, creates and compiles a 1D Convolutional Neural Network (CNN) for multi-label classification tasks.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "Input Parameters\n",
    "* `input_shape`: Specifies the shape of the input data. It represents (timesteps, features), where timesteps refer to the length of the time series (e.g., 50 windows), and features represent the number of measurements in each time step (e.g., accelerometer readings).\n",
    "* `num_activity_classes`: The number of output classes for the activity classification problem.\n",
    "* `num_social_signal_classes`: The number of output classes for the social signal classification problem.\n",
    "\n",
    "Returns\n",
    "* The function returns a compiled 1D CNN model with two outputs that is ready to be trained on your data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Function Breakdown\n",
    "1. **Model Initialization:**\n",
    "    * `inputs = Input(shape=input_shape)`: Initializes the input layer with the specified shape.\n",
    "2. **First Convolutional Layer:**\n",
    "    * `Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)`\n",
    "        * This is the first 1D convolutional layer.\n",
    "        * `filters=64`: The layer applies 64 filters (or kernels) over the input data.\n",
    "        * `kernel_size=3`: Each filter will cover 3 timesteps at a time (a window of 3).\n",
    "        * `activation='relu'`: The Rectified Linear Unit (ReLU) activation function introduces non-linearity and helps the model learn complex patterns.\n",
    "    * `MaxPooling1D(pool_size=2)(x)`: This pooling layer reduces the dimensionality of the data by taking the maximum value from each 2-timestep window (`pool_size=2`).\n",
    "3. **Second Convolutional Layer:**\n",
    "    * `Conv1D(filters=128, kernel_size=3, activation='relu')(x)`\n",
    "        * This is the second convolutional layer, similar to the first, but with 128 filters.\n",
    "        * `kernel_size=3` and `activation='relu'` function in the same way as the first Conv1D layer.\n",
    "    * `MaxPooling1D(pool_size=2)(x)`: Another pooling layer to downsample the output, further reducing the datas dimensionality.\n",
    "4. **Flattening Layer:**\n",
    "    * `Flatten()(x)`: Converts the 2D output of the convolutional and pooling layers into a 1D vector.\n",
    "5. **Fully Connected Layer:**\n",
    "    * `Dense(128, activation='relu')(x)`: This is a fully connected layer with 128 units/neurons.\n",
    "6. **Dropout Layer:**\n",
    "    * `Dropout(0.5)(x)`: This layer randomly sets 50% of the neurons to zero during training to prevent overfitting.\n",
    "7. **Output Layer for Activity Classification:**\n",
    "    * `Dense(num_activity_classes, activation='softmax', name='activity_output')(x)`: This is the output layer for activity classification with `num_activity_classes` neurons.\n",
    "8. **Output Layer for Social Signal Classification:**\n",
    "    * `Dense(num_social_signal_classes, activation='softmax', name='social_signal_output')(x)`: This is the output layer for social signal classification with `num_social_signal_classes` neurons.\n",
    "9. **Model Definition:**\n",
    "    * `model = Model(inputs=inputs, outputs=[activity_output, social_signal_output])`: Defines the model with two outputs.\n",
    "10. **Compiling the Model:**\n",
    "    * `model.compile(optimizer='adam', loss={'activity_output': 'categorical_crossentropy', 'social_signal_output': 'categorical_crossentropy'}, metrics={'activity_output': 'accuracy', 'social_signal_output': 'accuracy'})`\n",
    "        * Optimizer: 'adam': Adam is an optimization algorithm that adjusts the learning rate during training to improve performance.\n",
    "        * Loss: 'categorical_crossentropy': This loss function is used for multi-class classification problems where the target variable is one-hot encoded.\n",
    "        * Metrics: ['accuracy']: The accuracy metric is used to evaluate the models performance during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sCOkh99EOg8t"
   },
   "outputs": [],
   "source": [
    "def build_1d_cnn_model(input_shape, num_activity_classes, num_social_signal_classes):\n",
    "    \"\"\"\n",
    "    Builds and compiles a 1D CNN model for multi-label classification.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (timesteps, features).\n",
    "        num_activity_classes (int): The number of output activity classes.\n",
    "        num_social_signal_classes (int): The number of output social signal classes.\n",
    "\n",
    "    Returns:\n",
    "        model (Sequential): Compiled 1D CNN model with two outputs.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First Conv1D layer\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer for activity classification\n",
    "    activity_output = Dense(num_activity_classes, activation='softmax', name='activity_output')(x)\n",
    "\n",
    "    # Output layer for social signal classification\n",
    "    social_signal_output = Dense(num_social_signal_classes, activation='softmax', name='social_signal_output')(x)\n",
    "\n",
    "    # Define the model with two outputs\n",
    "    model = Model(inputs=inputs, outputs=[activity_output, social_signal_output])\n",
    "\n",
    "    # Compile the model with two separate losses\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'activity_output': 'categorical_crossentropy', 'social_signal_output': 'categorical_crossentropy'},\n",
    "                  metrics={'activity_output': 'accuracy', 'social_signal_output': 'accuracy'})\n",
    "\n",
    "    # Print a detailed summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HurfE6lmOjQT"
   },
   "source": [
    "# Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLs1eacYoa_S"
   },
   "source": [
    "## Step 1: Prepare and Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdGR352hph4X"
   },
   "source": [
    "Now the training and testing data will be created by calling the function `process_activity`. The `process_activity` function is used to generate sliding windows and labels for the training and testing sets.\n",
    "* `X_train` and `X_test` are 3D arrays of sliding windows (shape: num_windows, window_size, num_features).\n",
    "* `y_train_activity` and `y_train_social_signal` are 1D arrays of activity and social signal labels for the training set.\n",
    "* `y_test_activity` and `y_test_social_signal` are 1D arrays of activity and social signal labels for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OtpVBr4Fpq_8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48575 train windows generated with 48575 activity labels and 48575 social signal labels\n",
      "12154 test windows generated with 12154 activity labels and 12154 social signal labels\n"
     ]
    }
   ],
   "source": [
    "# Generate the sliding windows along with activity and social signal labels for training and testing sets\n",
    "X_train, y_train_activity, y_train_social_signal, X_test, y_test_activity, y_test_social_signal = process_activity(your_dataset_path, test_size=0.2, window_size=50, step_size=50)\n",
    "print(f\"{len(X_train)} train windows generated with {len(y_train_activity)} activity labels and {len(y_train_social_signal)} social signal labels\")\n",
    "print(f\"{len(X_test)} test windows generated with {len(y_test_activity)} activity labels and {len(y_test_social_signal)} social signal labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training and testing sets generated by the `process_activity` function are checked to see that they have the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymA3yh7YFKix",
    "outputId": "3682e518-cfe7-454a-e719-664a4c435732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (48575, 50, 3), y_train_activity shape: (48575,), y_train_social_signal shape: (48575,)\n",
      "X_test shape: (12154, 50, 3), y_test_activity shape: (12154,), y_test_social_signal shape: (12154,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and test arrays to verify that everything has been combined correctly\n",
    "print(f\"X_train shape: {X_train.shape}, y_train_activity shape: {y_train_activity.shape}, y_train_social_signal shape: {y_train_social_signal.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test_activity shape: {y_test_activity.shape}, y_test_social_signal shape: {y_test_social_signal.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yQGU1vwIQdz"
   },
   "source": [
    "### One-Hot Encode Labels (for multi-class classification)\n",
    "Since there are more than two classes, the labels must be one-hot encoded, especially as the model will use categorical cross-entropy loss.\n",
    "\n",
    "One-Hot Encoding converts categorical labels into binary vectors (one-hot encoded format). Each class label is represented as a binary vector with 1 for the correct class and 0 for others. This is necessary for training models that use categorical_crossentropy as the loss function, such as a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9b2J1EVdHj0U"
   },
   "outputs": [],
   "source": [
    "# Initialise the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit the OneHotEncoder on the training activity labels and transform them to one-hot encoded format\n",
    "y_train_activity_one_hot = encoder.fit_transform(y_train_activity.reshape(-1, 1))\n",
    "\n",
    "# Transform the test activity labels to one-hot encoded format using the already fitted encoder\n",
    "y_test_activity_one_hot = encoder.transform(y_test_activity.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Fit the OneHotEncoder on the training social signal labels and transform them to one-hot encoded format\n",
    "y_train_social_signal_one_hot = encoder.fit_transform(y_train_social_signal.reshape(-1, 1))\n",
    "\n",
    "# Transform the test social signal labels to one-hot encoded format using the already fitted encoder\n",
    "y_test_social_signal_one_hot = encoder.transform(y_test_social_signal.reshape(-1, 1))\n",
    "\n",
    "# Explanation:\n",
    "# - y_train_activity_one_hot, y_train_social_signal_one_hot, y_test_activity_one_hot and y_test_social_signal_one_hot are now 2D arrays where each row is a one-hot encoded binary vector corresponding to a class label.\n",
    "# - The number of columns in the one-hot encoded labels equals the number of unique classes (activities).\n",
    "# For example, if there are 6 unique activities, the encoded vector will have 6 elements, with a '1' indicating the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlnbOVr0rDbV",
    "outputId": "98ddbd62-4d6c-41ba-ac94-00d74a30f3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_activity_one_hot shape: (48575, 11), y_train_social_signal_one_hot shape: (48575, 4), y_test_activity_one_hot shape: (12154, 11), y_test_social_signal_one_hot shape: (12154, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the one-hot encoded labels to verify that the transformation was successful\n",
    "print(f\"y_train_activity_one_hot shape: {y_train_activity_one_hot.shape}, y_train_social_signal_one_hot shape: {y_train_social_signal_one_hot.shape}, y_test_activity_one_hot shape: {y_test_activity_one_hot.shape}, y_test_social_signal_one_hot shape: {y_test_social_signal_one_hot.shape}\")\n",
    "\n",
    "# Explanation of shapes:\n",
    "# - The shape of y_train_activity_one_hot will be (num_samples, num_classes), where:\n",
    "#     - num_samples is the number of training windows.\n",
    "#     - num_classes is the number of unique activities (the length of the one-hot vectors).\n",
    "# - Similarly, y_test_activity_one_hot will have the same number of columns (num_classes) as y_train_activity_one_hot but will have fewer rows (corresponding to the number of test windows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEhUxZzzJzzI"
   },
   "source": [
    "## Step 2: Build the 1D-CNN Model\n",
    "Call our `build_1d_cnn_model` function to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "4sDZWZH_KKBD",
    "outputId": "12cc6048-0921-414c-d2d7-b8d5268a8196"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d (\u001b[38;5;33mConv1D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_1[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the input shape for the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Determine the number of output classes (num_classes)\n",
    "num_activity_classes = y_train_activity_one_hot.shape[1]\n",
    "num_social_signal_classes = y_train_social_signal_one_hot.shape[1]\n",
    "\n",
    "# Build and compile the model\n",
    "# The function will return a compiled model ready for training\n",
    "model = build_1d_cnn_model(input_shape, num_activity_classes, num_social_signal_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1-SHEmtKM0D"
   },
   "source": [
    "## Step 3: Train the CNN Model\n",
    "\n",
    "Train the 1D CNN model using the training data and validate on the test data. The model will learn to map input sliding windows to their corresponding activity and social signal labels.\n",
    "\n",
    "`model.fit()` is used to train the neural network model. It takes several parameters:\n",
    "* `X_train`: The input training data (sliding windows), with shape (num_samples, window_size, num_features).\n",
    "* `{'activity_output': y_train_activity_one_hot, 'social_signal_output': y_train_social_signal_one_hot}`: The corresponding one-hot encoded labels for the training data, with shape (num_samples, num_classes).\n",
    "* `epochs`: Number of times the entire training dataset is passed through the model. In this case, we are training for 20 epochs, meaning the model will see the entire training set 20 times.\n",
    "* `batch_size`: Number of samples processed before the model's weights are updated. Here, the batch size is set to 32, meaning the model will process 32 samples at a time before updating its parameters.\n",
    "* `validation_data`: This parameter allows us to evaluate the model's performance on the test data after each epoch. It takes the test data and corresponding one-hot encoded test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4zICRasKPsT",
    "outputId": "c2c4bde2-1417-4c96-e25b-8e4f3d039bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - activity_output_accuracy: 0.8688 - activity_output_loss: 0.5125 - loss: 1.6992 - social_signal_output_accuracy: 0.5182 - social_signal_output_loss: 1.1867 - val_activity_output_accuracy: 0.9084 - val_activity_output_loss: 0.2955 - val_loss: 1.3331 - val_social_signal_output_accuracy: 0.5863 - val_social_signal_output_loss: 1.0371\n",
      "Epoch 2/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - activity_output_accuracy: 0.9165 - activity_output_loss: 0.2625 - loss: 1.2478 - social_signal_output_accuracy: 0.5904 - social_signal_output_loss: 0.9854 - val_activity_output_accuracy: 0.9264 - val_activity_output_loss: 0.2580 - val_loss: 1.1815 - val_social_signal_output_accuracy: 0.6052 - val_social_signal_output_loss: 0.9230\n",
      "Epoch 3/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9312 - activity_output_loss: 0.2186 - loss: 1.1519 - social_signal_output_accuracy: 0.5971 - social_signal_output_loss: 0.9333 - val_activity_output_accuracy: 0.9348 - val_activity_output_loss: 0.2415 - val_loss: 1.1366 - val_social_signal_output_accuracy: 0.6058 - val_social_signal_output_loss: 0.8947\n",
      "Epoch 4/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9423 - activity_output_loss: 0.1869 - loss: 1.0968 - social_signal_output_accuracy: 0.5963 - social_signal_output_loss: 0.9098 - val_activity_output_accuracy: 0.9458 - val_activity_output_loss: 0.2200 - val_loss: 1.0843 - val_social_signal_output_accuracy: 0.6122 - val_social_signal_output_loss: 0.8640\n",
      "Epoch 5/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9445 - activity_output_loss: 0.1715 - loss: 1.0591 - social_signal_output_accuracy: 0.6072 - social_signal_output_loss: 0.8877 - val_activity_output_accuracy: 0.9352 - val_activity_output_loss: 0.2556 - val_loss: 1.1601 - val_social_signal_output_accuracy: 0.5936 - val_social_signal_output_loss: 0.9040\n",
      "Epoch 6/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9483 - activity_output_loss: 0.1616 - loss: 1.0268 - social_signal_output_accuracy: 0.6121 - social_signal_output_loss: 0.8651 - val_activity_output_accuracy: 0.9460 - val_activity_output_loss: 0.2447 - val_loss: 1.1084 - val_social_signal_output_accuracy: 0.6117 - val_social_signal_output_loss: 0.8634\n",
      "Epoch 7/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9532 - activity_output_loss: 0.1450 - loss: 1.0018 - social_signal_output_accuracy: 0.6125 - social_signal_output_loss: 0.8569 - val_activity_output_accuracy: 0.9473 - val_activity_output_loss: 0.2540 - val_loss: 1.0966 - val_social_signal_output_accuracy: 0.6130 - val_social_signal_output_loss: 0.8423\n",
      "Epoch 8/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9590 - activity_output_loss: 0.1315 - loss: 0.9799 - social_signal_output_accuracy: 0.6155 - social_signal_output_loss: 0.8484 - val_activity_output_accuracy: 0.9536 - val_activity_output_loss: 0.2634 - val_loss: 1.1117 - val_social_signal_output_accuracy: 0.6139 - val_social_signal_output_loss: 0.8479\n",
      "Epoch 9/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9595 - activity_output_loss: 0.1286 - loss: 0.9602 - social_signal_output_accuracy: 0.6259 - social_signal_output_loss: 0.8316 - val_activity_output_accuracy: 0.9498 - val_activity_output_loss: 0.2691 - val_loss: 1.1144 - val_social_signal_output_accuracy: 0.6084 - val_social_signal_output_loss: 0.8450\n",
      "Epoch 10/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9627 - activity_output_loss: 0.1169 - loss: 0.9338 - social_signal_output_accuracy: 0.6272 - social_signal_output_loss: 0.8169 - val_activity_output_accuracy: 0.9519 - val_activity_output_loss: 0.3043 - val_loss: 1.1780 - val_social_signal_output_accuracy: 0.6221 - val_social_signal_output_loss: 0.8733\n",
      "Epoch 11/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9660 - activity_output_loss: 0.1116 - loss: 0.9107 - social_signal_output_accuracy: 0.6308 - social_signal_output_loss: 0.7991 - val_activity_output_accuracy: 0.9539 - val_activity_output_loss: 0.2848 - val_loss: 1.1385 - val_social_signal_output_accuracy: 0.6188 - val_social_signal_output_loss: 0.8533\n",
      "Epoch 12/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - activity_output_accuracy: 0.9678 - activity_output_loss: 0.1053 - loss: 0.8993 - social_signal_output_accuracy: 0.6370 - social_signal_output_loss: 0.7940 - val_activity_output_accuracy: 0.9557 - val_activity_output_loss: 0.2769 - val_loss: 1.1146 - val_social_signal_output_accuracy: 0.6331 - val_social_signal_output_loss: 0.8373\n",
      "Epoch 13/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9694 - activity_output_loss: 0.0972 - loss: 0.8732 - social_signal_output_accuracy: 0.6434 - social_signal_output_loss: 0.7760 - val_activity_output_accuracy: 0.9510 - val_activity_output_loss: 0.3073 - val_loss: 1.1222 - val_social_signal_output_accuracy: 0.6368 - val_social_signal_output_loss: 0.8144\n",
      "Epoch 14/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9700 - activity_output_loss: 0.0937 - loss: 0.8435 - social_signal_output_accuracy: 0.6588 - social_signal_output_loss: 0.7498 - val_activity_output_accuracy: 0.9547 - val_activity_output_loss: 0.3106 - val_loss: 1.1211 - val_social_signal_output_accuracy: 0.6479 - val_social_signal_output_loss: 0.8101\n",
      "Epoch 15/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9726 - activity_output_loss: 0.0880 - loss: 0.8335 - social_signal_output_accuracy: 0.6558 - social_signal_output_loss: 0.7455 - val_activity_output_accuracy: 0.9590 - val_activity_output_loss: 0.2868 - val_loss: 1.0747 - val_social_signal_output_accuracy: 0.6580 - val_social_signal_output_loss: 0.7875\n",
      "Epoch 16/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9765 - activity_output_loss: 0.0808 - loss: 0.8110 - social_signal_output_accuracy: 0.6699 - social_signal_output_loss: 0.7302 - val_activity_output_accuracy: 0.9566 - val_activity_output_loss: 0.3381 - val_loss: 1.1429 - val_social_signal_output_accuracy: 0.6508 - val_social_signal_output_loss: 0.8043\n",
      "Epoch 17/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9768 - activity_output_loss: 0.0803 - loss: 0.8027 - social_signal_output_accuracy: 0.6706 - social_signal_output_loss: 0.7225 - val_activity_output_accuracy: 0.9589 - val_activity_output_loss: 0.3336 - val_loss: 1.1307 - val_social_signal_output_accuracy: 0.6609 - val_social_signal_output_loss: 0.7967\n",
      "Epoch 18/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9774 - activity_output_loss: 0.0725 - loss: 0.7746 - social_signal_output_accuracy: 0.6811 - social_signal_output_loss: 0.7021 - val_activity_output_accuracy: 0.9539 - val_activity_output_loss: 0.3448 - val_loss: 1.1532 - val_social_signal_output_accuracy: 0.6626 - val_social_signal_output_loss: 0.8079\n",
      "Epoch 19/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - activity_output_accuracy: 0.9778 - activity_output_loss: 0.0713 - loss: 0.7729 - social_signal_output_accuracy: 0.6790 - social_signal_output_loss: 0.7016 - val_activity_output_accuracy: 0.9541 - val_activity_output_loss: 0.3728 - val_loss: 1.1739 - val_social_signal_output_accuracy: 0.6602 - val_social_signal_output_loss: 0.8006\n",
      "Epoch 20/20\n",
      "\u001b[1m1518/1518\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9786 - activity_output_loss: 0.0695 - loss: 0.7648 - social_signal_output_accuracy: 0.6867 - social_signal_output_loss: 0.6952 - val_activity_output_accuracy: 0.9589 - val_activity_output_loss: 0.3766 - val_loss: 1.1580 - val_social_signal_output_accuracy: 0.6683 - val_social_signal_output_loss: 0.7811\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,  # Input data\n",
    "    {'activity_output': y_train_activity_one_hot, 'social_signal_output': y_train_social_signal_one_hot},  # Target labels\n",
    "    epochs=20, # Train the model for 20 epochs\n",
    "    batch_size=32, # Use a batch size of 32\n",
    "    validation_data=(X_test, {'activity_output': y_test_activity_one_hot, 'social_signal_output': y_test_social_signal_one_hot}) # Validate the test set after each epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`val_accuracy` is the accuracy of the model on the validation data (in this case X_test, y_test_activity_one_hot and y_test_social_signal_one_hot). The `accuracy` is the training accuracy for the current epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrSPBPh4KcBn"
   },
   "source": [
    "## Step 4: Evaluate the Model\n",
    "After training, the model is evaluated on the test set. This is done with a classification report and 5-Fold Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB2Bi7ieKelv",
    "outputId": "de615d87-81e5-45df-f50b-60b21139fdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Classification Report for Activity Labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9036    0.8333    0.8671       270\n",
      "           1     0.7552    0.5370    0.6277       270\n",
      "           2     0.9737    0.9923    0.9829      3887\n",
      "           3     0.6458    0.6889    0.6667       270\n",
      "           4     0.7348    0.8099    0.7705       284\n",
      "           5     0.9765    1.0000    0.9881      1659\n",
      "           6     0.9898    0.9946    0.9922      1660\n",
      "           7     0.9975    0.9729    0.9850      1658\n",
      "           8     0.9868    0.9897    0.9883      1658\n",
      "           9     0.8125    0.8246    0.8185       268\n",
      "          10     0.9741    0.8370    0.9004       270\n",
      "\n",
      "    accuracy                         0.9589     12154\n",
      "   macro avg     0.8864    0.8618    0.8716     12154\n",
      "weighted avg     0.9585    0.9589    0.9580     12154\n",
      "\n",
      "Classification Report for Social Signal Labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7947    0.7063    0.7479      3337\n",
      "           1     0.6301    0.6770    0.6527      1613\n",
      "           2     0.5267    0.1936    0.2832      1632\n",
      "           3     0.6357    0.7821    0.7014      5572\n",
      "\n",
      "    accuracy                         0.6683     12154\n",
      "   macro avg     0.6468    0.5898    0.5963     12154\n",
      "weighted avg     0.6640    0.6683    0.6515     12154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Seperate the predictions for activity and social signal\n",
    "y_pred_probs_activity = y_pred_probs[0]\n",
    "y_pred_probs_social_signal = y_pred_probs[1]\n",
    "\n",
    "# Convert the predicted probabilities to class labels (taking the argmax of the probabilities)\n",
    "y_pred_classes_activity = np.argmax(y_pred_probs_activity, axis=1)\n",
    "y_pred_classes_social_signal = np.argmax(y_pred_probs_social_signal, axis=1)\n",
    "\n",
    "# Convert the true test labels from one-hot encoding back to class labels\n",
    "y_true_classes_activity = np.argmax(y_test_activity_one_hot, axis=1)\n",
    "y_true_classes_social_signal = np.argmax(y_test_social_signal_oane_hot, axis=1)\n",
    "\n",
    "# Generate the classification report for activity labels\n",
    "report_activity = classification_report(y_true_classes_activity, y_pred_classes_activity, digits=4)\n",
    "print(\"Classification Report for Activity Labels:\")\n",
    "print(report_activity)\n",
    "\n",
    "# Generate the classification report for social signal labels\n",
    "report_social_signal = classification_report(y_true_classes_social_signal, y_pred_classes_social_signal, digits=4)\n",
    "print(\"Classification Report for Social Signal Labels:\")\n",
    "print(report_social_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer_1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " max_pooling1d_3      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d_2[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " max_pooling1d_3      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_3[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.8607 - activity_output_loss: 0.5476 - loss: 1.7491 - social_signal_output_accuracy: 0.5100 - social_signal_output_loss: 1.2015 - val_activity_output_accuracy: 0.9180 - val_activity_output_loss: 0.2627 - val_loss: 1.2829 - val_social_signal_output_accuracy: 0.5843 - val_social_signal_output_loss: 1.0200\n",
      "Epoch 2/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9125 - activity_output_loss: 0.2783 - loss: 1.2964 - social_signal_output_accuracy: 0.5890 - social_signal_output_loss: 1.0181 - val_activity_output_accuracy: 0.9268 - val_activity_output_loss: 0.2200 - val_loss: 1.1606 - val_social_signal_output_accuracy: 0.6037 - val_social_signal_output_loss: 0.9401\n",
      "Epoch 3/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9263 - activity_output_loss: 0.2311 - loss: 1.1799 - social_signal_output_accuracy: 0.5938 - social_signal_output_loss: 0.9488 - val_activity_output_accuracy: 0.9406 - val_activity_output_loss: 0.1856 - val_loss: 1.0869 - val_social_signal_output_accuracy: 0.6093 - val_social_signal_output_loss: 0.9009\n",
      "Epoch 4/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9382 - activity_output_loss: 0.1961 - loss: 1.1184 - social_signal_output_accuracy: 0.5987 - social_signal_output_loss: 0.9223 - val_activity_output_accuracy: 0.9442 - val_activity_output_loss: 0.1643 - val_loss: 1.0370 - val_social_signal_output_accuracy: 0.6117 - val_social_signal_output_loss: 0.8721\n",
      "Epoch 5/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9420 - activity_output_loss: 0.1854 - loss: 1.0939 - social_signal_output_accuracy: 0.6024 - social_signal_output_loss: 0.9085 - val_activity_output_accuracy: 0.9506 - val_activity_output_loss: 0.1518 - val_loss: 1.0234 - val_social_signal_output_accuracy: 0.6080 - val_social_signal_output_loss: 0.8710\n",
      "Epoch 6/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9471 - activity_output_loss: 0.1618 - loss: 1.0565 - social_signal_output_accuracy: 0.5997 - social_signal_output_loss: 0.8947 - val_activity_output_accuracy: 0.9502 - val_activity_output_loss: 0.1430 - val_loss: 0.9742 - val_social_signal_output_accuracy: 0.6187 - val_social_signal_output_loss: 0.8307\n",
      "Epoch 7/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9497 - activity_output_loss: 0.1532 - loss: 1.0298 - social_signal_output_accuracy: 0.6042 - social_signal_output_loss: 0.8767 - val_activity_output_accuracy: 0.9544 - val_activity_output_loss: 0.1421 - val_loss: 0.9658 - val_social_signal_output_accuracy: 0.6186 - val_social_signal_output_loss: 0.8233\n",
      "Epoch 8/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9549 - activity_output_loss: 0.1412 - loss: 1.0055 - social_signal_output_accuracy: 0.6050 - social_signal_output_loss: 0.8644 - val_activity_output_accuracy: 0.9547 - val_activity_output_loss: 0.1362 - val_loss: 0.9586 - val_social_signal_output_accuracy: 0.6215 - val_social_signal_output_loss: 0.8220\n",
      "Epoch 9/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - activity_output_accuracy: 0.9582 - activity_output_loss: 0.1278 - loss: 0.9772 - social_signal_output_accuracy: 0.6070 - social_signal_output_loss: 0.8494 - val_activity_output_accuracy: 0.9610 - val_activity_output_loss: 0.1170 - val_loss: 0.9206 - val_social_signal_output_accuracy: 0.6211 - val_social_signal_output_loss: 0.8033\n",
      "Epoch 10/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9588 - activity_output_loss: 0.1204 - loss: 0.9549 - social_signal_output_accuracy: 0.6167 - social_signal_output_loss: 0.8345 - val_activity_output_accuracy: 0.9655 - val_activity_output_loss: 0.1089 - val_loss: 0.9158 - val_social_signal_output_accuracy: 0.6246 - val_social_signal_output_loss: 0.8066\n",
      "Epoch 11/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9629 - activity_output_loss: 0.1139 - loss: 0.9496 - social_signal_output_accuracy: 0.6123 - social_signal_output_loss: 0.8357 - val_activity_output_accuracy: 0.9663 - val_activity_output_loss: 0.1072 - val_loss: 0.9028 - val_social_signal_output_accuracy: 0.6316 - val_social_signal_output_loss: 0.7953\n",
      "Epoch 12/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9650 - activity_output_loss: 0.1075 - loss: 0.9321 - social_signal_output_accuracy: 0.6211 - social_signal_output_loss: 0.8246 - val_activity_output_accuracy: 0.9663 - val_activity_output_loss: 0.1096 - val_loss: 0.8993 - val_social_signal_output_accuracy: 0.6295 - val_social_signal_output_loss: 0.7895\n",
      "Epoch 13/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9675 - activity_output_loss: 0.1016 - loss: 0.9219 - social_signal_output_accuracy: 0.6188 - social_signal_output_loss: 0.8202 - val_activity_output_accuracy: 0.9675 - val_activity_output_loss: 0.0976 - val_loss: 0.8596 - val_social_signal_output_accuracy: 0.6378 - val_social_signal_output_loss: 0.7618\n",
      "Epoch 14/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9709 - activity_output_loss: 0.0923 - loss: 0.8965 - social_signal_output_accuracy: 0.6302 - social_signal_output_loss: 0.8042 - val_activity_output_accuracy: 0.9646 - val_activity_output_loss: 0.1160 - val_loss: 0.8796 - val_social_signal_output_accuracy: 0.6380 - val_social_signal_output_loss: 0.7639\n",
      "Epoch 15/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9700 - activity_output_loss: 0.0929 - loss: 0.8832 - social_signal_output_accuracy: 0.6354 - social_signal_output_loss: 0.7903 - val_activity_output_accuracy: 0.9686 - val_activity_output_loss: 0.0956 - val_loss: 0.8607 - val_social_signal_output_accuracy: 0.6454 - val_social_signal_output_loss: 0.7650\n",
      "Epoch 16/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9706 - activity_output_loss: 0.0890 - loss: 0.8768 - social_signal_output_accuracy: 0.6332 - social_signal_output_loss: 0.7878 - val_activity_output_accuracy: 0.9683 - val_activity_output_loss: 0.0986 - val_loss: 0.8570 - val_social_signal_output_accuracy: 0.6444 - val_social_signal_output_loss: 0.7589\n",
      "Epoch 17/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9740 - activity_output_loss: 0.0811 - loss: 0.8573 - social_signal_output_accuracy: 0.6430 - social_signal_output_loss: 0.7762 - val_activity_output_accuracy: 0.9703 - val_activity_output_loss: 0.0987 - val_loss: 0.8541 - val_social_signal_output_accuracy: 0.6574 - val_social_signal_output_loss: 0.7554\n",
      "Epoch 18/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9745 - activity_output_loss: 0.0802 - loss: 0.8441 - social_signal_output_accuracy: 0.6494 - social_signal_output_loss: 0.7640 - val_activity_output_accuracy: 0.9697 - val_activity_output_loss: 0.1014 - val_loss: 0.8550 - val_social_signal_output_accuracy: 0.6600 - val_social_signal_output_loss: 0.7537\n",
      "Epoch 19/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9756 - activity_output_loss: 0.0751 - loss: 0.8340 - social_signal_output_accuracy: 0.6488 - social_signal_output_loss: 0.7588 - val_activity_output_accuracy: 0.9712 - val_activity_output_loss: 0.0989 - val_loss: 0.8045 - val_social_signal_output_accuracy: 0.6715 - val_social_signal_output_loss: 0.7059\n",
      "Epoch 20/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9763 - activity_output_loss: 0.0720 - loss: 0.8172 - social_signal_output_accuracy: 0.6527 - social_signal_output_loss: 0.7452 - val_activity_output_accuracy: 0.9712 - val_activity_output_loss: 0.0993 - val_loss: 0.8058 - val_social_signal_output_accuracy: 0.6866 - val_social_signal_output_loss: 0.7066\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Completed fold 1\n",
      "Training fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer_2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_4      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " max_pooling1d_5      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_4      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d_4[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " max_pooling1d_5      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_5[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - activity_output_accuracy: 0.8645 - activity_output_loss: 0.5406 - loss: 1.7391 - social_signal_output_accuracy: 0.5120 - social_signal_output_loss: 1.1985 - val_activity_output_accuracy: 0.9199 - val_activity_output_loss: 0.2636 - val_loss: 1.3191 - val_social_signal_output_accuracy: 0.5767 - val_social_signal_output_loss: 1.0558\n",
      "Epoch 2/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9130 - activity_output_loss: 0.2784 - loss: 1.3183 - social_signal_output_accuracy: 0.5809 - social_signal_output_loss: 1.0399 - val_activity_output_accuracy: 0.9340 - val_activity_output_loss: 0.2058 - val_loss: 1.2070 - val_social_signal_output_accuracy: 0.5992 - val_social_signal_output_loss: 1.0008\n",
      "Epoch 3/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9256 - activity_output_loss: 0.2352 - loss: 1.1966 - social_signal_output_accuracy: 0.5941 - social_signal_output_loss: 0.9614 - val_activity_output_accuracy: 0.9390 - val_activity_output_loss: 0.1924 - val_loss: 1.1128 - val_social_signal_output_accuracy: 0.5968 - val_social_signal_output_loss: 0.9200\n",
      "Epoch 4/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9330 - activity_output_loss: 0.2101 - loss: 1.1413 - social_signal_output_accuracy: 0.5966 - social_signal_output_loss: 0.9312 - val_activity_output_accuracy: 0.9493 - val_activity_output_loss: 0.1569 - val_loss: 1.0277 - val_social_signal_output_accuracy: 0.6095 - val_social_signal_output_loss: 0.8704\n",
      "Epoch 5/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9382 - activity_output_loss: 0.1905 - loss: 1.0972 - social_signal_output_accuracy: 0.6043 - social_signal_output_loss: 0.9067 - val_activity_output_accuracy: 0.9539 - val_activity_output_loss: 0.1399 - val_loss: 1.0065 - val_social_signal_output_accuracy: 0.6135 - val_social_signal_output_loss: 0.8663\n",
      "Epoch 6/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9448 - activity_output_loss: 0.1688 - loss: 1.0608 - social_signal_output_accuracy: 0.6025 - social_signal_output_loss: 0.8920 - val_activity_output_accuracy: 0.9541 - val_activity_output_loss: 0.1408 - val_loss: 0.9881 - val_social_signal_output_accuracy: 0.6137 - val_social_signal_output_loss: 0.8472\n",
      "Epoch 7/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9497 - activity_output_loss: 0.1579 - loss: 1.0438 - social_signal_output_accuracy: 0.6053 - social_signal_output_loss: 0.8859 - val_activity_output_accuracy: 0.9598 - val_activity_output_loss: 0.1271 - val_loss: 0.9731 - val_social_signal_output_accuracy: 0.6159 - val_social_signal_output_loss: 0.8456\n",
      "Epoch 8/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9506 - activity_output_loss: 0.1470 - loss: 1.0075 - social_signal_output_accuracy: 0.6088 - social_signal_output_loss: 0.8605 - val_activity_output_accuracy: 0.9605 - val_activity_output_loss: 0.1178 - val_loss: 0.9433 - val_social_signal_output_accuracy: 0.6187 - val_social_signal_output_loss: 0.8251\n",
      "Epoch 9/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9574 - activity_output_loss: 0.1368 - loss: 0.9942 - social_signal_output_accuracy: 0.6118 - social_signal_output_loss: 0.8574 - val_activity_output_accuracy: 0.9643 - val_activity_output_loss: 0.1127 - val_loss: 0.9493 - val_social_signal_output_accuracy: 0.6138 - val_social_signal_output_loss: 0.8363\n",
      "Epoch 10/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9578 - activity_output_loss: 0.1275 - loss: 0.9752 - social_signal_output_accuracy: 0.6163 - social_signal_output_loss: 0.8477 - val_activity_output_accuracy: 0.9677 - val_activity_output_loss: 0.0987 - val_loss: 0.9214 - val_social_signal_output_accuracy: 0.6230 - val_social_signal_output_loss: 0.8225\n",
      "Epoch 11/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9603 - activity_output_loss: 0.1203 - loss: 0.9574 - social_signal_output_accuracy: 0.6168 - social_signal_output_loss: 0.8372 - val_activity_output_accuracy: 0.9660 - val_activity_output_loss: 0.1077 - val_loss: 0.8995 - val_social_signal_output_accuracy: 0.6186 - val_social_signal_output_loss: 0.7914\n",
      "Epoch 12/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9644 - activity_output_loss: 0.1105 - loss: 0.9468 - social_signal_output_accuracy: 0.6140 - social_signal_output_loss: 0.8363 - val_activity_output_accuracy: 0.9698 - val_activity_output_loss: 0.0927 - val_loss: 0.8704 - val_social_signal_output_accuracy: 0.6326 - val_social_signal_output_loss: 0.7774\n",
      "Epoch 13/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9675 - activity_output_loss: 0.1028 - loss: 0.9290 - social_signal_output_accuracy: 0.6181 - social_signal_output_loss: 0.8263 - val_activity_output_accuracy: 0.9646 - val_activity_output_loss: 0.1111 - val_loss: 0.9297 - val_social_signal_output_accuracy: 0.6200 - val_social_signal_output_loss: 0.8183\n",
      "Epoch 14/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9661 - activity_output_loss: 0.1052 - loss: 0.9240 - social_signal_output_accuracy: 0.6199 - social_signal_output_loss: 0.8188 - val_activity_output_accuracy: 0.9659 - val_activity_output_loss: 0.1071 - val_loss: 0.9045 - val_social_signal_output_accuracy: 0.6359 - val_social_signal_output_loss: 0.7973\n",
      "Epoch 15/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9711 - activity_output_loss: 0.0974 - loss: 0.9113 - social_signal_output_accuracy: 0.6250 - social_signal_output_loss: 0.8138 - val_activity_output_accuracy: 0.9689 - val_activity_output_loss: 0.0989 - val_loss: 0.8828 - val_social_signal_output_accuracy: 0.6335 - val_social_signal_output_loss: 0.7836\n",
      "Epoch 16/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9711 - activity_output_loss: 0.0900 - loss: 0.8967 - social_signal_output_accuracy: 0.6321 - social_signal_output_loss: 0.8067 - val_activity_output_accuracy: 0.9670 - val_activity_output_loss: 0.1038 - val_loss: 0.8757 - val_social_signal_output_accuracy: 0.6412 - val_social_signal_output_loss: 0.7716\n",
      "Epoch 17/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9707 - activity_output_loss: 0.0879 - loss: 0.8762 - social_signal_output_accuracy: 0.6355 - social_signal_output_loss: 0.7883 - val_activity_output_accuracy: 0.9713 - val_activity_output_loss: 0.0879 - val_loss: 0.8413 - val_social_signal_output_accuracy: 0.6453 - val_social_signal_output_loss: 0.7531\n",
      "Epoch 18/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9741 - activity_output_loss: 0.0799 - loss: 0.8568 - social_signal_output_accuracy: 0.6412 - social_signal_output_loss: 0.7770 - val_activity_output_accuracy: 0.9732 - val_activity_output_loss: 0.0860 - val_loss: 0.8396 - val_social_signal_output_accuracy: 0.6537 - val_social_signal_output_loss: 0.7534\n",
      "Epoch 19/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9738 - activity_output_loss: 0.0803 - loss: 0.8488 - social_signal_output_accuracy: 0.6439 - social_signal_output_loss: 0.7685 - val_activity_output_accuracy: 0.9752 - val_activity_output_loss: 0.0839 - val_loss: 0.8431 - val_social_signal_output_accuracy: 0.6611 - val_social_signal_output_loss: 0.7589\n",
      "Epoch 20/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9765 - activity_output_loss: 0.0755 - loss: 0.8410 - social_signal_output_accuracy: 0.6492 - social_signal_output_loss: 0.7655 - val_activity_output_accuracy: 0.9735 - val_activity_output_loss: 0.0861 - val_loss: 0.8286 - val_social_signal_output_accuracy: 0.6705 - val_social_signal_output_loss: 0.7423\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Completed fold 2\n",
      "Training fold 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer_3        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " max_pooling1d_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_3        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d_6[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " max_pooling1d_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_7[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - activity_output_accuracy: 0.8634 - activity_output_loss: 0.5559 - loss: 1.7616 - social_signal_output_accuracy: 0.5222 - social_signal_output_loss: 1.2057 - val_activity_output_accuracy: 0.9046 - val_activity_output_loss: 0.2981 - val_loss: 1.3602 - val_social_signal_output_accuracy: 0.5706 - val_social_signal_output_loss: 1.0614\n",
      "Epoch 2/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9030 - activity_output_loss: 0.3005 - loss: 1.3276 - social_signal_output_accuracy: 0.5794 - social_signal_output_loss: 1.0271 - val_activity_output_accuracy: 0.9273 - val_activity_output_loss: 0.2183 - val_loss: 1.1506 - val_social_signal_output_accuracy: 0.6067 - val_social_signal_output_loss: 0.9317\n",
      "Epoch 3/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9237 - activity_output_loss: 0.2418 - loss: 1.1866 - social_signal_output_accuracy: 0.5982 - social_signal_output_loss: 0.9448 - val_activity_output_accuracy: 0.9453 - val_activity_output_loss: 0.1715 - val_loss: 1.0633 - val_social_signal_output_accuracy: 0.6131 - val_social_signal_output_loss: 0.8911\n",
      "Epoch 4/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9377 - activity_output_loss: 0.2024 - loss: 1.1188 - social_signal_output_accuracy: 0.6006 - social_signal_output_loss: 0.9164 - val_activity_output_accuracy: 0.9500 - val_activity_output_loss: 0.1580 - val_loss: 1.0298 - val_social_signal_output_accuracy: 0.6105 - val_social_signal_output_loss: 0.8712\n",
      "Epoch 5/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9447 - activity_output_loss: 0.1815 - loss: 1.0845 - social_signal_output_accuracy: 0.6022 - social_signal_output_loss: 0.9030 - val_activity_output_accuracy: 0.9548 - val_activity_output_loss: 0.1412 - val_loss: 0.9931 - val_social_signal_output_accuracy: 0.6162 - val_social_signal_output_loss: 0.8514\n",
      "Epoch 6/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9480 - activity_output_loss: 0.1632 - loss: 1.0463 - social_signal_output_accuracy: 0.6057 - social_signal_output_loss: 0.8832 - val_activity_output_accuracy: 0.9571 - val_activity_output_loss: 0.1316 - val_loss: 0.9907 - val_social_signal_output_accuracy: 0.6196 - val_social_signal_output_loss: 0.8585\n",
      "Epoch 7/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9495 - activity_output_loss: 0.1548 - loss: 1.0225 - social_signal_output_accuracy: 0.6133 - social_signal_output_loss: 0.8677 - val_activity_output_accuracy: 0.9618 - val_activity_output_loss: 0.1217 - val_loss: 0.9795 - val_social_signal_output_accuracy: 0.6107 - val_social_signal_output_loss: 0.8572\n",
      "Epoch 8/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9558 - activity_output_loss: 0.1391 - loss: 0.9891 - social_signal_output_accuracy: 0.6141 - social_signal_output_loss: 0.8499 - val_activity_output_accuracy: 0.9599 - val_activity_output_loss: 0.1227 - val_loss: 0.9475 - val_social_signal_output_accuracy: 0.6197 - val_social_signal_output_loss: 0.8242\n",
      "Epoch 9/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9593 - activity_output_loss: 0.1292 - loss: 0.9678 - social_signal_output_accuracy: 0.6232 - social_signal_output_loss: 0.8386 - val_activity_output_accuracy: 0.9640 - val_activity_output_loss: 0.1071 - val_loss: 0.9156 - val_social_signal_output_accuracy: 0.6248 - val_social_signal_output_loss: 0.8081\n",
      "Epoch 10/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9617 - activity_output_loss: 0.1264 - loss: 0.9563 - social_signal_output_accuracy: 0.6224 - social_signal_output_loss: 0.8299 - val_activity_output_accuracy: 0.9652 - val_activity_output_loss: 0.1101 - val_loss: 0.9009 - val_social_signal_output_accuracy: 0.6285 - val_social_signal_output_loss: 0.7904\n",
      "Epoch 11/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9640 - activity_output_loss: 0.1136 - loss: 0.9319 - social_signal_output_accuracy: 0.6275 - social_signal_output_loss: 0.8183 - val_activity_output_accuracy: 0.9696 - val_activity_output_loss: 0.0959 - val_loss: 0.8625 - val_social_signal_output_accuracy: 0.6416 - val_social_signal_output_loss: 0.7660\n",
      "Epoch 12/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9678 - activity_output_loss: 0.1043 - loss: 0.9088 - social_signal_output_accuracy: 0.6275 - social_signal_output_loss: 0.8045 - val_activity_output_accuracy: 0.9682 - val_activity_output_loss: 0.0994 - val_loss: 0.8562 - val_social_signal_output_accuracy: 0.6434 - val_social_signal_output_loss: 0.7563\n",
      "Epoch 13/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9676 - activity_output_loss: 0.1017 - loss: 0.8832 - social_signal_output_accuracy: 0.6390 - social_signal_output_loss: 0.7815 - val_activity_output_accuracy: 0.9704 - val_activity_output_loss: 0.0995 - val_loss: 0.8386 - val_social_signal_output_accuracy: 0.6615 - val_social_signal_output_loss: 0.7386\n",
      "Epoch 14/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9704 - activity_output_loss: 0.0961 - loss: 0.8735 - social_signal_output_accuracy: 0.6430 - social_signal_output_loss: 0.7774 - val_activity_output_accuracy: 0.9715 - val_activity_output_loss: 0.0956 - val_loss: 0.8289 - val_social_signal_output_accuracy: 0.6611 - val_social_signal_output_loss: 0.7329\n",
      "Epoch 15/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9719 - activity_output_loss: 0.0884 - loss: 0.8469 - social_signal_output_accuracy: 0.6484 - social_signal_output_loss: 0.7584 - val_activity_output_accuracy: 0.9717 - val_activity_output_loss: 0.0899 - val_loss: 0.8186 - val_social_signal_output_accuracy: 0.6726 - val_social_signal_output_loss: 0.7281\n",
      "Epoch 16/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9738 - activity_output_loss: 0.0856 - loss: 0.8435 - social_signal_output_accuracy: 0.6517 - social_signal_output_loss: 0.7579 - val_activity_output_accuracy: 0.9727 - val_activity_output_loss: 0.1038 - val_loss: 0.8470 - val_social_signal_output_accuracy: 0.6707 - val_social_signal_output_loss: 0.7427\n",
      "Epoch 17/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9727 - activity_output_loss: 0.0839 - loss: 0.8258 - social_signal_output_accuracy: 0.6583 - social_signal_output_loss: 0.7419 - val_activity_output_accuracy: 0.9720 - val_activity_output_loss: 0.0937 - val_loss: 0.8022 - val_social_signal_output_accuracy: 0.6807 - val_social_signal_output_loss: 0.7079\n",
      "Epoch 18/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9750 - activity_output_loss: 0.0824 - loss: 0.8107 - social_signal_output_accuracy: 0.6686 - social_signal_output_loss: 0.7282 - val_activity_output_accuracy: 0.9717 - val_activity_output_loss: 0.0953 - val_loss: 0.7991 - val_social_signal_output_accuracy: 0.6879 - val_social_signal_output_loss: 0.7036\n",
      "Epoch 19/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9762 - activity_output_loss: 0.0798 - loss: 0.8050 - social_signal_output_accuracy: 0.6665 - social_signal_output_loss: 0.7252 - val_activity_output_accuracy: 0.9745 - val_activity_output_loss: 0.0885 - val_loss: 0.7701 - val_social_signal_output_accuracy: 0.6921 - val_social_signal_output_loss: 0.6813\n",
      "Epoch 20/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9773 - activity_output_loss: 0.0726 - loss: 0.7879 - social_signal_output_accuracy: 0.6735 - social_signal_output_loss: 0.7153 - val_activity_output_accuracy: 0.9752 - val_activity_output_loss: 0.0881 - val_loss: 0.7788 - val_social_signal_output_accuracy: 0.6824 - val_social_signal_output_loss: 0.6901\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Completed fold 3\n",
      "Training fold 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer_4        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " max_pooling1d_9      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_4        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d_8[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " max_pooling1d_9      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_9[\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " dense_4 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.8607 - activity_output_loss: 0.5569 - loss: 1.7702 - social_signal_output_accuracy: 0.5091 - social_signal_output_loss: 1.2133 - val_activity_output_accuracy: 0.9165 - val_activity_output_loss: 0.2573 - val_loss: 1.2923 - val_social_signal_output_accuracy: 0.5785 - val_social_signal_output_loss: 1.0348\n",
      "Epoch 2/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9056 - activity_output_loss: 0.2937 - loss: 1.3146 - social_signal_output_accuracy: 0.5858 - social_signal_output_loss: 1.0209 - val_activity_output_accuracy: 0.9387 - val_activity_output_loss: 0.2014 - val_loss: 1.1451 - val_social_signal_output_accuracy: 0.5953 - val_social_signal_output_loss: 0.9434\n",
      "Epoch 3/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9283 - activity_output_loss: 0.2304 - loss: 1.1708 - social_signal_output_accuracy: 0.5969 - social_signal_output_loss: 0.9405 - val_activity_output_accuracy: 0.9504 - val_activity_output_loss: 0.1678 - val_loss: 1.0830 - val_social_signal_output_accuracy: 0.6076 - val_social_signal_output_loss: 0.9147\n",
      "Epoch 4/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9399 - activity_output_loss: 0.1945 - loss: 1.1057 - social_signal_output_accuracy: 0.6000 - social_signal_output_loss: 0.9112 - val_activity_output_accuracy: 0.9571 - val_activity_output_loss: 0.1499 - val_loss: 1.0173 - val_social_signal_output_accuracy: 0.6165 - val_social_signal_output_loss: 0.8669\n",
      "Epoch 5/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9465 - activity_output_loss: 0.1682 - loss: 1.0639 - social_signal_output_accuracy: 0.6011 - social_signal_output_loss: 0.8956 - val_activity_output_accuracy: 0.9598 - val_activity_output_loss: 0.1386 - val_loss: 0.9783 - val_social_signal_output_accuracy: 0.6131 - val_social_signal_output_loss: 0.8392\n",
      "Epoch 6/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9507 - activity_output_loss: 0.1530 - loss: 1.0154 - social_signal_output_accuracy: 0.6115 - social_signal_output_loss: 0.8625 - val_activity_output_accuracy: 0.9607 - val_activity_output_loss: 0.1329 - val_loss: 0.9764 - val_social_signal_output_accuracy: 0.6167 - val_social_signal_output_loss: 0.8430\n",
      "Epoch 7/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9557 - activity_output_loss: 0.1392 - loss: 1.0015 - social_signal_output_accuracy: 0.6092 - social_signal_output_loss: 0.8622 - val_activity_output_accuracy: 0.9636 - val_activity_output_loss: 0.1268 - val_loss: 0.9346 - val_social_signal_output_accuracy: 0.6206 - val_social_signal_output_loss: 0.8075\n",
      "Epoch 8/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9563 - activity_output_loss: 0.1384 - loss: 0.9739 - social_signal_output_accuracy: 0.6234 - social_signal_output_loss: 0.8355 - val_activity_output_accuracy: 0.9658 - val_activity_output_loss: 0.1240 - val_loss: 0.9415 - val_social_signal_output_accuracy: 0.6262 - val_social_signal_output_loss: 0.8169\n",
      "Epoch 9/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9617 - activity_output_loss: 0.1215 - loss: 0.9658 - social_signal_output_accuracy: 0.6171 - social_signal_output_loss: 0.8443 - val_activity_output_accuracy: 0.9648 - val_activity_output_loss: 0.1254 - val_loss: 0.9493 - val_social_signal_output_accuracy: 0.6238 - val_social_signal_output_loss: 0.8236\n",
      "Epoch 10/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9639 - activity_output_loss: 0.1110 - loss: 0.9484 - social_signal_output_accuracy: 0.6213 - social_signal_output_loss: 0.8373 - val_activity_output_accuracy: 0.9704 - val_activity_output_loss: 0.1055 - val_loss: 0.9110 - val_social_signal_output_accuracy: 0.6269 - val_social_signal_output_loss: 0.8049\n",
      "Epoch 11/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9677 - activity_output_loss: 0.1046 - loss: 0.9233 - social_signal_output_accuracy: 0.6288 - social_signal_output_loss: 0.8187 - val_activity_output_accuracy: 0.9653 - val_activity_output_loss: 0.1275 - val_loss: 0.9301 - val_social_signal_output_accuracy: 0.6340 - val_social_signal_output_loss: 0.8020\n",
      "Epoch 12/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - activity_output_accuracy: 0.9685 - activity_output_loss: 0.1015 - loss: 0.9084 - social_signal_output_accuracy: 0.6341 - social_signal_output_loss: 0.8068 - val_activity_output_accuracy: 0.9715 - val_activity_output_loss: 0.1042 - val_loss: 0.8632 - val_social_signal_output_accuracy: 0.6408 - val_social_signal_output_loss: 0.7584\n",
      "Epoch 13/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9711 - activity_output_loss: 0.0927 - loss: 0.8865 - social_signal_output_accuracy: 0.6354 - social_signal_output_loss: 0.7938 - val_activity_output_accuracy: 0.9712 - val_activity_output_loss: 0.1048 - val_loss: 0.8720 - val_social_signal_output_accuracy: 0.6427 - val_social_signal_output_loss: 0.7666\n",
      "Epoch 14/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9717 - activity_output_loss: 0.0905 - loss: 0.8773 - social_signal_output_accuracy: 0.6408 - social_signal_output_loss: 0.7868 - val_activity_output_accuracy: 0.9706 - val_activity_output_loss: 0.1079 - val_loss: 0.8740 - val_social_signal_output_accuracy: 0.6502 - val_social_signal_output_loss: 0.7655\n",
      "Epoch 15/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9755 - activity_output_loss: 0.0819 - loss: 0.8671 - social_signal_output_accuracy: 0.6414 - social_signal_output_loss: 0.7851 - val_activity_output_accuracy: 0.9733 - val_activity_output_loss: 0.1000 - val_loss: 0.8345 - val_social_signal_output_accuracy: 0.6632 - val_social_signal_output_loss: 0.7340\n",
      "Epoch 16/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9748 - activity_output_loss: 0.0810 - loss: 0.8514 - social_signal_output_accuracy: 0.6488 - social_signal_output_loss: 0.7704 - val_activity_output_accuracy: 0.9734 - val_activity_output_loss: 0.0980 - val_loss: 0.8656 - val_social_signal_output_accuracy: 0.6426 - val_social_signal_output_loss: 0.7670\n",
      "Epoch 17/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9767 - activity_output_loss: 0.0748 - loss: 0.8368 - social_signal_output_accuracy: 0.6551 - social_signal_output_loss: 0.7620 - val_activity_output_accuracy: 0.9747 - val_activity_output_loss: 0.1053 - val_loss: 0.8512 - val_social_signal_output_accuracy: 0.6501 - val_social_signal_output_loss: 0.7455\n",
      "Epoch 18/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9771 - activity_output_loss: 0.0779 - loss: 0.8373 - social_signal_output_accuracy: 0.6547 - social_signal_output_loss: 0.7594 - val_activity_output_accuracy: 0.9746 - val_activity_output_loss: 0.1033 - val_loss: 0.8596 - val_social_signal_output_accuracy: 0.6552 - val_social_signal_output_loss: 0.7561\n",
      "Epoch 19/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9786 - activity_output_loss: 0.0712 - loss: 0.8223 - social_signal_output_accuracy: 0.6600 - social_signal_output_loss: 0.7510 - val_activity_output_accuracy: 0.9765 - val_activity_output_loss: 0.1049 - val_loss: 0.8154 - val_social_signal_output_accuracy: 0.6658 - val_social_signal_output_loss: 0.7099\n",
      "Epoch 20/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - activity_output_accuracy: 0.9810 - activity_output_loss: 0.0648 - loss: 0.8094 - social_signal_output_accuracy: 0.6623 - social_signal_output_loss: 0.7446 - val_activity_output_accuracy: 0.9758 - val_activity_output_loss: 0.1021 - val_loss: 0.8481 - val_social_signal_output_accuracy: 0.6512 - val_social_signal_output_loss: 0.7454\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Completed fold 4\n",
      "Training fold 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer_5        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " max_pooling1d_10     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling1d_10 \n",
       "\n",
       " max_pooling1d_11     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
       "\n",
       " flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling1d_11 \n",
       "\n",
       " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span>  flatten_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " activity_output      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span>  dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       " social_signal_outp  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>  dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_5        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m640\u001b[0m  input_layer_5[\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " max_pooling1d_10     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m0\u001b[0m  conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m24,704\u001b[0m  max_pooling1d_10 \n",
       "\n",
       " max_pooling1d_11     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
       "\n",
       " flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)                \u001b[38;5;34m0\u001b[0m  max_pooling1d_11 \n",
       "\n",
       " dense_5 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m180,352\u001b[0m  flatten_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dropout_5 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " activity_output      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)              \u001b[38;5;34m1,419\u001b[0m  dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n",
       " social_signal_outp  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m516\u001b[0m  dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,631</span> (811.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,631\u001b[0m (811.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - activity_output_accuracy: 0.8630 - activity_output_loss: 0.5369 - loss: 1.7344 - social_signal_output_accuracy: 0.5158 - social_signal_output_loss: 1.1975 - val_activity_output_accuracy: 0.9142 - val_activity_output_loss: 0.2605 - val_loss: 1.3060 - val_social_signal_output_accuracy: 0.5545 - val_social_signal_output_loss: 1.0450\n",
      "Epoch 2/20\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - activity_output_accuracy: 0.9152 - activity_output_loss: 0.2724 - loss: 1.2678 - social_signal_output_accuracy: 0.5885 - social_signal_output_loss: 0.9955 - val_activity_output_accuracy: 0.9335 - val_activity_output_loss: 0.2139 - val_loss: 1.1350 - val_social_signal_output_accuracy: 0.5950 - val_social_signal_output_loss: 0.9206\n",
      "Epoch 3/20\n",
      "\u001b[1m 244/1215\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - activity_output_accuracy: 0.9319 - activity_output_loss: 0.2143 - loss: 1.1412 - social_signal_output_accuracy: 0.5987 - social_signal_output_loss: 0.9269"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38104\\479787883.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Build and compile the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_1d_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_activity_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_social_signal_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Train the model on this fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mX_train_fold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;33m{\u001b[0m\u001b[1;34m'activity_output'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train_activity_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'social_signal_output'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train_social_signal_fold\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# One-indexed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\utils\\progbar.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33mf\"\u001b[0m\u001b[1;33m - \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                     avg = backend.convert_to_numpy(\n\u001b[1;32m--> 163\u001b[1;33m                         backend.numpy.mean(\n\u001b[0m\u001b[0;32m    164\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"int\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bool\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mori_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m     output = tf.reduce_mean(\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2548\u001b[0m   \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2549\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   2550\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2551\u001b[0m       gen_math_ops.mean(\n\u001b[1;32m-> 2552\u001b[1;33m           \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2553\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2052\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2053\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m       \u001b[1;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2023\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2027\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\seanc\\miniconda3\\envs\\pdiot\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   8836\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Range\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8837\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8838\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8839\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8840\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8841\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8842\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8843\u001b[0m       return _range_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up KFold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialise lists to store the accuracy of each fold\n",
    "activity_accuracy = []\n",
    "social_signal_accuracy = []\n",
    "\n",
    "# Perform KFold cross-validation\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train):\n",
    "    print(f\"Training fold {fold_no}\")\n",
    "\n",
    "    # Split the data for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_activity_fold, y_val_activity_fold = y_train_activity_one_hot[train_index], y_train_activity_one_hot[val_index]\n",
    "    y_train_social_signal_fold, y_val_social_signal_fold = y_train_social_signal_one_hot[train_index], y_train_social_signal_one_hot[val_index]\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_1d_cnn_model(input_shape, num_activity_classes, num_social_signal_classes)\n",
    "\n",
    "    # Train the model on this fold\n",
    "    history = model.fit(\n",
    "        X_train_fold,\n",
    "        {'activity_output': y_train_activity_fold, 'social_signal_output': y_train_social_signal_fold},\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_fold, {'activity_output': y_val_activity_fold, 'social_signal_output': y_val_social_signal_fold})\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred_probs = model.predict(X_val_fold)\n",
    "    y_pred_probs_activity = np.argmax(y_pred_probs[0], axis = 1)\n",
    "    y_pred_probs_social_signal = np.argmax(y_pred_probs[1], axis = 1)\n",
    "\n",
    "    y_true_activity = np.argmax(y_val_activity_fold, axis = 1)\n",
    "    y_true_social_signal = np.argmax(y_val_social_signal_fold, axis = 1)\n",
    "\n",
    "    # Generate the classification report for activity and social signal labels\n",
    "    report_activity = classification_report(y_true_activity, y_pred_probs_activity, output_dict=True)\n",
    "    report_social_signal = classification_report(y_true_social_signal, y_pred_probs_social_signal, output_dict=True)\n",
    "\n",
    "    # Append the reports to the lists for averaging later\n",
    "    activity_accuracy.append(report_activity['accuracy'])\n",
    "    social_signal_accuracy.append(report_social_signal['accuracy'])\n",
    "\n",
    "    print(f\"Completed fold {fold_no}\")\n",
    "    fold_no += 1\n",
    "\n",
    "average_activity_accuracy = np.mean(activity_accuracy)\n",
    "average_social_signal_accuracy = np.mean(social_signal_accuracy)\n",
    "\n",
    "print(f\"Average Activity Accuracy: {average_activity_accuracy:.4f}\")\n",
    "print(f\"Average Social Signal Accuracy: {average_social_signal_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHNzGOHDtnQ4"
   },
   "source": [
    "# Exporting your model to TFLite\n",
    "\n",
    "You can use the TFLiteConverter class provided by TensorFlow to convert your trained model into the TensorFlow Lite format. We export models to TensorFlow Lite (TFLite) for several reasons, primarily because TFLite is designed for deployment on edge devices, such as mobile phones, embedded systems, IoT devices, and microcontrollers, where computational resources and power are limited. This is necessary as you will be running your ML models on your Android devices to perform live classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_QKoJtmufDa",
    "outputId": "48d281ab-4ed3-4ee1-fd47-30f7f4c10b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\seanc\\AppData\\Local\\Temp\\tmpcjr4lwin\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\seanc\\AppData\\Local\\Temp\\tmpcjr4lwin\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\seanc\\AppData\\Local\\Temp\\tmpcjr4lwin'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  List[TensorSpec(shape=(None, 11), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  1387511079824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511079632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511082128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511082320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511080976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511082896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511083472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511085584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511083088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1387511084048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model successfully exported to model.tflite\n"
     ]
    }
   ],
   "source": [
    "# Convert the trained Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)  # model is your trained Keras model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a .tflite file\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model successfully exported to model.tflite\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pdiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
